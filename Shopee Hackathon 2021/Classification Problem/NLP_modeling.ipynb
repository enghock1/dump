{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shopee 2021 Hackathon - Address Elements Extraction\n",
    "\n",
    "### The goal of this competition is to build a model to correctly extract Point of Interest (POI) Names and Street Names from unformatted Indonesia addresses.\n",
    "\n",
    "### For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.util import everygrams,bigrams\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## CLASS DEFINITION FOR PREPROCESS LOGIC #######################\n",
    "\n",
    "REPLACE_WITH_STRING = \",\"\n",
    "\n",
    "class PreProcessLogic:\n",
    "    def process(self, string: str):\n",
    "        pass\n",
    "\n",
    "class RukunTetanggaPreProcessLogic(PreProcessLogic):\n",
    "    def process(self, string: str):\n",
    "        return re.sub(r\"\\brt(\\.)*(( )*[0-9]+)+\", REPLACE_WITH_STRING, string)\n",
    "\n",
    "class RukunWargaPreProcessLogic(PreProcessLogic):\n",
    "    def process(self, string: str):\n",
    "        return re.sub(r\"\\brw(\\.)*(( )*[0-9]+)+\", REPLACE_WITH_STRING, string)\n",
    "\n",
    "class PostalCodePreProcessLogic(PreProcessLogic):\n",
    "    def process(self, string: str):\n",
    "        return re.sub(r\"\\b[0-9]{5}[^,]*\", REPLACE_WITH_STRING, string)\n",
    "    \n",
    "class NumberCodePreprocessLogic(PreProcessLogic):\n",
    "    def process(self, string: str):\n",
    "        return re.sub(r\"\\bno[0-9\\-]*\\b(\\.)*(( )*([a-z]{0,2}( )*[0-9\\-]+[a-z]{0,2}|[0-9\\-][a-z]{1})\\b)*\", REPLACE_WITH_STRING, string)\n",
    "\n",
    "\n",
    "class PreprocessingHelper:\n",
    "    def generate_all_n_grams(self, string: str):\n",
    "        result = []\n",
    "        for part in string.split(\",\"):\n",
    "            part = part.strip()\n",
    "            result.extend([' '.join(x) for x in everygrams(part.split())])\n",
    "\n",
    "        return np.asarray(result)  \n",
    "\n",
    "    def generate_all_bi_grams(self, string: str):\n",
    "        result = []\n",
    "        if \"/\" in string:\n",
    "            for part in string.split(\"/\"):\n",
    "                if part:\n",
    "                    if \" \" not in part:\n",
    "                        result.append(part)\n",
    "                    else:\n",
    "                        part = part.strip()\n",
    "                        result.extend([' '.join(x) for x in bigrams(part.split())])\n",
    "            return result\n",
    "        else:\n",
    "            str_list = string.split()\n",
    "            if len(str_list) == 1:\n",
    "                return str_list\n",
    "            return [' '.join(x) for x in bigrams(str_list)]\n",
    "\n",
    "\n",
    "    def generate_all_n_grams_labels(self, n_grams: list, labels: list): \n",
    "        labels_for_n_gram = np.zeros(shape=(len(n_grams),len(labels)))\n",
    "\n",
    "        for i in range(len(n_grams)):\n",
    "            for j in range(len(labels)):\n",
    "                # use prefix matching for all words\n",
    "                original_string = re.escape(n_grams[i]).replace(\"\\ \", \" \")\n",
    "                prefix_regex = \"^\" + re.sub(r\"\\s+\", r\"[^\\\\s]*\\\\s+\", original_string)\n",
    "                prefix_match_result = re.search(prefix_regex, labels[j])\n",
    "                if prefix_match_result:\n",
    "                    labels_for_n_gram[i][j] = 1\n",
    "\n",
    "        return labels_for_n_gram\n",
    "\n",
    "    def preprocesing_step(self, ids: list, raw_address: list, answers: list):\n",
    "        processing_logics = [RukunTetanggaPreProcessLogic(), RukunWargaPreProcessLogic(), PostalCodePreProcessLogic(), NumberCodePreprocessLogic()]\n",
    "\n",
    "        processed_address = []\n",
    "\n",
    "        for addr in raw_address:\n",
    "            new_addr = addr\n",
    "            for logic in processing_logics:\n",
    "                new_addr = logic.process(new_addr)\n",
    "            processed_address.append(new_addr)\n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(len(processed_address)):\n",
    "            x = self.generate_all_n_grams(processed_address[i])\n",
    "            X.extend([[ids[i], x_i] for x_i in x])\n",
    "            y = []\n",
    "            if i < len(answers):\n",
    "                y = self.generate_all_n_grams_labels(x, answers[i].split(\"/\"))\n",
    "                Y.extend(y)\n",
    "\n",
    "        return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define encoding functions\n",
    "\n",
    "class bag_of_word_encoder:\n",
    "    \n",
    "    def __init__(self, kmean_n=None):\n",
    "        self.kmean_n = kmean_n\n",
    "        \n",
    "        \n",
    "    def get_unique_words(self, data_np):\n",
    "        \n",
    "        # get unique words\n",
    "        unique_words = []\n",
    "        for address in data_np:\n",
    "            unique_words.extend(address.split(' '))\n",
    "            \n",
    "        unique_words = list(set(unique_words))\n",
    "        \n",
    "        # edge case: remove empty element\n",
    "        #unique_words.remove('')\n",
    "        \n",
    "        return unique_words\n",
    "            \n",
    "    \n",
    "    def get_feature_dimension(self, unique_words):\n",
    "        \n",
    "        # get unique character\n",
    "        unique_character = set()\n",
    "        for unique in unique_words:\n",
    "            if len(unique) != 0:\n",
    "                for u in list(unique):\n",
    "                    unique_character.add(u)\n",
    "        unique_character = list(unique_character)\n",
    "                               \n",
    "        # encode each word by the unique character\n",
    "        X = self.one_hot_encoder(unique_words, unique_character = unique_character)\n",
    "        \n",
    "        return X, unique_character\n",
    "\n",
    "    \n",
    "    def one_hot_encoder(self, unique_words, unique_character=None):\n",
    "        \n",
    "        if unique_character == None:\n",
    "            unique_character = self.unique_character_\n",
    "        \n",
    "        X = np.zeros((len(unique_words),len(unique_character)), dtype=int)\n",
    "        for i, word in enumerate(unique_words):\n",
    "            for u in list(word):\n",
    "                c = [j for j, item in enumerate(unique_character) if item in set(u)]\n",
    "                X[i,c[0]] = 1    \n",
    "      \n",
    "        return X\n",
    "                               \n",
    "    \n",
    "    def train_Kmeans(self, X, n_init=10, max_iter=500):\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=self.kmean_n, n_init=n_init, max_iter=max_iter)\n",
    "        kmeans.fit(X)\n",
    "\n",
    "        clusters_dimension = kmeans.cluster_centers_\n",
    "\n",
    "        return clusters_dimension\n",
    "    \n",
    "    \n",
    "    def build_dictionary(self, data_np, load_kmean=False):\n",
    "        \n",
    "        # get all unique words from data\n",
    "        unique_words = self.get_unique_words(data_np)\n",
    "         \n",
    "        # get one-hot-encoder X\n",
    "        X, unique_character = self.get_feature_dimension(unique_words)\n",
    "    \n",
    "        if load_kmean == False:                      \n",
    "                               \n",
    "            # perform k-mean clustering to clusters unique words\n",
    "            print('Begin computing K-Means clustering...', end='')\n",
    "            clusters_dimension = self.train_Kmeans(X)\n",
    "            np.savetxt('kMeansCluster.out', clusters_dimension, delimiter=',')\n",
    "            print('Complete!')\n",
    "            \n",
    "        else:\n",
    "            # import kmean result\n",
    "            clusters_dimension = np.loadtxt('kMeansCluster.out', delimiter=',')\n",
    "        \n",
    "        # save result in class\n",
    "        self.clusters_dimension_ = clusters_dimension\n",
    "        self.unique_words_ = unique_words\n",
    "        self.unique_character_ = unique_character\n",
    "    \n",
    "    \n",
    "    def compute_bow(self, data_np):\n",
    "        \n",
    "        bow_features = np.zeros((data_np.shape[0], self.clusters_dimension_.shape[0]))\n",
    "\n",
    "        # fit the kmean model\n",
    "        NN = NearestNeighbors(n_neighbors=1)\n",
    "        NN.fit(self.clusters_dimension_)        \n",
    "        \n",
    "        for i, address in enumerate(data_np):\n",
    "            words = address.split(' ')\n",
    "            features = self.one_hot_encoder(words)\n",
    "            \n",
    "            # find the nearest neighbors\n",
    "            _, indices = NN.kneighbors(features)\n",
    "            \n",
    "            # create bins\n",
    "            bins = [i for i in range(0,self.clusters_dimension_.shape[0]+1)]\n",
    "            \n",
    "            # compute histogram\n",
    "            bow_feature, _ = np.histogram(indices, density=False, bins=bins)\n",
    "            \n",
    "            bow_features[i,:] = bow_feature\n",
    "            \n",
    "    \n",
    "        return bow_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train RF entirely\n",
    "def train_RF_all(X, y, kfold, max_leaf_nodes, maxFeature, numTrees, criterions):\n",
    "\n",
    "    # split materials data into tranining and test set\n",
    "    X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, stratify=y, test_size=0.20)\n",
    "\n",
    "    trn_A = np.zeros((len(max_leaf_nodes),))\n",
    "    val_A = np.zeros((len(max_leaf_nodes),))\n",
    "\n",
    "    # for each maxDepths\n",
    "    for i, max_leaf_node in enumerate(max_leaf_nodes):      \n",
    "\n",
    "        # k-fold validation\n",
    "        skf = StratifiedKFold(n_splits=kfold)\n",
    "        for lrn_index, val_index in skf.split(X_trn, y_trn):\n",
    "            X_lrn, X_val = X_trn[lrn_index], X_trn[val_index]\n",
    "            y_lrn, y_val = y_trn[lrn_index], y_trn[val_index]\n",
    "\n",
    "            ## perform RF\n",
    "            trnA, valA, _ = train_RF(X_lrn, y_lrn, X_val, y_val, numTrees, \n",
    "                                     maxFeature, max_leaf_nodes=max_leaf_node)\n",
    "            trn_A[i] += trnA\n",
    "            val_A[i] += valA\n",
    "                \n",
    "    # averaging training and validation accuracy\n",
    "    trn_A /= kfold\n",
    "    val_A /= kfold\n",
    "    \n",
    "    # print averaged training and validation accuracy\n",
    "    print('Averaged training Accuracy:')\n",
    "    print(trn_A)\n",
    "    print('Averaged validation Accuracy:')\n",
    "    print(val_A)\n",
    "    \n",
    "    # find the optimal model for each cases\n",
    "    idx = np.where(val_A == val_A.max())\n",
    "    opt_max_leaf_nodes = max_leaf_nodes[idx[0][0]]\n",
    "\n",
    "    # calculate class imbalance weight \n",
    "    weight = {1 : sum(y_trn==-1)/sum(y_trn==1)}\n",
    "\n",
    "    ## Training and testing on optimal models\n",
    "    trn_A, tst_A, opt_model = train_RF(X_trn, y_trn, X_tst, y_tst, numTrees, maxFeature,\n",
    "                                       max_leaf_nodes=opt_max_leaf_nodes, weights=weight)\n",
    "    print('Standard RF \\nTrain_A: %f \\n Test_A: %f\\n' % (trn_A, tst_A))\n",
    "\n",
    "    return opt_model\n",
    "\n",
    "\n",
    "def train_RF(X_trn, y_trn, X_tst, y_tst, numTree, maxFeature, max_leaf_nodes=3, \n",
    "             criterion='gini', weights=None):\n",
    "    \n",
    "    model = BalancedRandomForestClassifier(n_estimators=numTree, max_features=maxFeature, max_leaf_nodes=max_leaf_nodes, \n",
    "                                           criterion=criterion, n_jobs=15)     \n",
    "    model.fit(X_trn,y_trn)\n",
    "    \n",
    "    ypred = model.predict(X_trn)\n",
    "    trn_A = accuracy(y_trn, ypred, weights)\n",
    "    \n",
    "    ypred = model.predict(X_tst)\n",
    "    tst_A = accuracy(y_tst, ypred, weights)\n",
    "    \n",
    "    return trn_A, tst_A, model\n",
    "\n",
    "\n",
    "def accuracy(ytrue, ypred, weights=None):\n",
    "    \"\"\"Calculate normalized accuracy\n",
    "    Input:\n",
    "    ytrue [n,]     : n array containing true y\n",
    "    ypred [n,]     : n array containing predicted y\n",
    "    weights [dict] : dict containing the specified class weight. If None, \n",
    "                     the weight will be extracted from the trained RF classs.\n",
    "\n",
    "    Output:\n",
    "    accuracy [int]: accuracy value\n",
    "    \"\"\"\n",
    "\n",
    "    cls = np.unique(ytrue)     \n",
    "    if weights == None:\n",
    "        weight = {cls[0] : 1}\n",
    "    else:\n",
    "        weight = weights\n",
    "\n",
    "    R = list(weight.values())[0]\n",
    "    tn, fp, fn, tp = confusion_matrix(ytrue, ypred).ravel()\n",
    "\n",
    "    if [*weight] == cls[0]:  # if key of weight is equal to class 0\n",
    "        error = (R*fp + fn) / ((fn + tp) + R*(tn + fp))\n",
    "    else: \n",
    "        error = (fp + R*fn) / (R*(fn + tp) + (tn + fp))\n",
    "\n",
    "    return 1 - error\n",
    "\n",
    "\n",
    "def predict(model, ids, X_string, X, previous_ypreds=[]):\n",
    "    \n",
    "    # find unique id\n",
    "    ids_int = np.array(list(map(int, ids)))\n",
    "    unique_ids = np.unique(ids_int)\n",
    "    \n",
    "    if len(previous_ypreds) != 0:\n",
    "        idx_previous_ypreds = np.where(previous_ypreds==1)[0]\n",
    "        ids_int = np.delete(ids_int, idx_previous_ypreds, axis = 0)\n",
    "        X = np.delete(X, idx_previous_ypreds, axis = 0)\n",
    "        X_string = np.delete(X_string, idx_previous_ypreds, axis = 0)\n",
    "        \n",
    "    result = []\n",
    "    \n",
    "    # predict X test \n",
    "    ypreds = model.predict(X) \n",
    "    \n",
    "    # for each unique id, find the \n",
    "    for Id in unique_ids:\n",
    "        idx_subset = np.where(ids_int==Id)[0]\n",
    "        \n",
    "        ypred = ypreds[idx_subset]\n",
    "    \n",
    "        if sum(ypred) == 0:\n",
    "            result.append('')\n",
    "            \n",
    "        elif sum(ypred) == 1:\n",
    "            idx = np.where(ypred==1)[0][0]\n",
    "            result.append(X_string[idx_subset[idx]])\n",
    "            \n",
    "        else:\n",
    "            Idx = np.where(ypred==1)[0]\n",
    "            counts = np.zeros((len(Idx),))\n",
    "            for i, idx in enumerate(Idx):\n",
    "                counts[i] = len(X_string[idx_subset[idx]])\n",
    "            result.append(X_string[idx_subset[Idx[np.argmax(counts)]]])\n",
    "            \n",
    "    return unique_ids, result, ypreds\n",
    "\n",
    "\n",
    "def write_results_to_csv(n, poi_ids, answers_poi, street_ids, answers_street, file_name):\n",
    "    empty_strings = [[\"\",\"\"]] * n\n",
    "    original_df = pd.DataFrame(empty_strings, columns=['POI', 'street'])\n",
    "    for idx, poi_id in enumerate(poi_ids):\n",
    "        original_df.iloc[poi_id,0] = answers_poi[idx]\n",
    "    for idx, street_id in enumerate(street_ids):\n",
    "        original_df.iloc[street_id,1] = answers_street[idx]\n",
    "\n",
    "    export_df = pd.DataFrame(empty_strings, columns=['id', 'POI/street'])\n",
    "    export_df['id'] =original_df.index\n",
    "    export_df['POI/street'] = original_df[['POI', 'street']].apply(lambda x: '/'.join(x), axis=1)\n",
    "    export_df.to_csv(file_name, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_csv(ids, answers_poi, answers_street, file_name):\n",
    "    data = []\n",
    "    for i in range(len(ids)):\n",
    "        answer_string = answers_poi[i].strip() + \"/\" + answers_street[i].strip()\n",
    "        data.append([ids[i], answer_string])\n",
    "    df = pd.DataFrame(data, columns=['id', 'POI/street'])\n",
    "    df.to_csv(file_name, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>raw_address</th>\n",
       "      <th>POI/street</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>jl kapuk timur delta sili iii lippo cika 11 a ...</td>\n",
       "      <td>/jl kapuk timur delta sili iii lippo cika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>aye, jati sampurna</td>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>setu siung 119 rt 5 1 13880 cipayung</td>\n",
       "      <td>/siung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>toko dita, kertosono</td>\n",
       "      <td>toko dita/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>jl. orde baru</td>\n",
       "      <td>/jl. orde baru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        raw_address  \\\n",
       "0   0  jl kapuk timur delta sili iii lippo cika 11 a ...   \n",
       "1   1                                 aye, jati sampurna   \n",
       "2   2               setu siung 119 rt 5 1 13880 cipayung   \n",
       "3   3                               toko dita, kertosono   \n",
       "4   4                                      jl. orde baru   \n",
       "\n",
       "                                  POI/street  \n",
       "0  /jl kapuk timur delta sili iii lippo cika  \n",
       "1                                          /  \n",
       "2                                     /siung  \n",
       "3                                 toko dita/  \n",
       "4                             /jl. orde baru  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import training data\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "ids = df['id'].values.tolist()\n",
    "raw_address = df['raw_address'].values.tolist()\n",
    "answers = df['POI/street'].values.tolist()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>raw_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s. par 53 sidanegara 4 cilacap tengah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>angg per, baloi indah kel. lubuk baja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>asma laun, mand imog,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ud agung rej, raya nga sri wedari karanganyar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>cut mutia, 35 baiturrahman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                    raw_address\n",
       "0   0          s. par 53 sidanegara 4 cilacap tengah\n",
       "1   1          angg per, baloi indah kel. lubuk baja\n",
       "2   2                          asma laun, mand imog,\n",
       "3   3  ud agung rej, raya nga sri wedari karanganyar\n",
       "4   4                     cut mutia, 35 baiturrahman"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import test data\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "ids_test = df_test['id'].values.tolist()\n",
    "raw_address_test = df_test['raw_address'].values.tolist()\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tuning parameter ###\n",
    "\n",
    "# number of K-means cluster\n",
    "n_cluster = 3000\n",
    "\n",
    "# sample size ratio\n",
    "ratio = 0.25\n",
    "\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing train step\n",
    "helper = PreprocessingHelper()\n",
    "X_ngram, y_ngram = helper.preprocesing_step(ids, raw_address, answers)\n",
    "X_ngram = X_ngram[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing test step\n",
    "X_ngram_test, _ = helper.preprocesing_step(ids_test, raw_address_test, [])\n",
    "X_ids_test = X_ngram_test[:,0]\n",
    "X_ngram_test = X_ngram_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin computing K-Means clustering...Complete!\n"
     ]
    }
   ],
   "source": [
    "# combine training and test input samples to build unique word dictionary\n",
    "X_ngram_dictionary = np.concatenate((X_ngram, X_ngram_test))\n",
    "\n",
    "# perform features reduction via K-mean clustering\n",
    "bow = bag_of_word_encoder(kmean_n = n_cluster)\n",
    "bow.build_dictionary(X_ngram_dictionary, load_kmean=False)\n",
    "\n",
    "X = bow.compute_bow(X_ngram)\n",
    "\n",
    "# compute Kmeans for test set\n",
    "X_test = bow.compute_bow(X_ngram_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate y labels\n",
    "y_POI = y_ngram[:,0]\n",
    "y_sn  = y_ngram[:,1]\n",
    "\n",
    "# reduce training set\n",
    "_, X_POI_new, _, y_POI_new = train_test_split(X, y_POI, stratify=y_POI, test_size=ratio)\n",
    "\n",
    "# reduce test set\n",
    "_, X_sn_new, _, y_sn_new = train_test_split(X, y_sn, stratify=y_sn, test_size=ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove variables\n",
    "del X_ngram_dictionary, df, ids, raw_address, answers, X_ngram, y_ngram, bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter\n",
    "kfold = 10\n",
    "max_leaf_nodes = [3000,5000,8000,10000,15000,20000,30000,50000,70000]\n",
    "maxFeature = int(np.sqrt(X_POI_new.shape[1]))\n",
    "numTrees = 200\n",
    "criterions = 'gini'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POI problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged training Accuracy:\n",
      "[0.7573245  0.76452191 0.77191332 0.77550945 0.78243037 0.78436049\n",
      " 0.78354664 0.78353235]\n",
      "Averaged validation Accuracy:\n",
      "[0.75251857 0.75751394 0.76229055 0.76452176 0.76901865 0.76879858\n",
      " 0.76649578 0.76654748]\n",
      "Standard RF \n",
      "Train_A: 0.774615 \n",
      " Test_A: 0.767309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train classification\n",
    "POI_model = train_RF_all(X_POI_new, y_POI_new, kfold, max_leaf_nodes, maxFeature, numTrees, criterions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "pickle.dump(POI_model, open('model_POI.sav', 'wb'))\n",
    "#POI_model = pickle.load(open('model_POI.sav', 'rb')) # to load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street name problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged training Accuracy:\n",
      "[0.73361242 0.73234516 0.73247273 0.73251883 0.73243222 0.73248746]\n",
      "Averaged validation Accuracy:\n",
      "[0.71426981 0.71000094 0.71013749 0.71012689 0.70981269 0.70985379]\n",
      "Standard RF \n",
      "Train_A: 0.710871 \n",
      " Test_A: 0.698670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train classification\n",
    "SN_model = train_RF_all(X_sn_new, y_sn_new, kfold, max_leaf_nodes, maxFeature, numTrees, criterions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(SN_model, open('model_SN.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin predict POI test...end!\n",
      "Begin predict SN test...end!\n"
     ]
    }
   ],
   "source": [
    "# predict test POI \n",
    "print('Begin predict POI test...', end='')\n",
    "idx_POI, result_POI, ypreds_POI = predict(POI_model, X_ids_test, X_ngram_test, X_test)\n",
    "print('end!')\n",
    "\n",
    "# predict test SN\n",
    "print('Begin predict SN test...', end='')\n",
    "idx_SN, result_SN, _ = predict(SN_model, X_ids_test, X_ngram_test, X_test, ypreds_POI)\n",
    "print('end!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save result \n",
    "write_results_to_csv(df_test.shape[0], idx_POI, result_POI, idx_SN, result_SN, 'test_prediction_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
